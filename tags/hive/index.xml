<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hive on codecat&#39;blog</title>
    <link>http://localhost:1313/tags/hive/</link>
    <description>Recent content in Hive on codecat&#39;blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 19 Jul 2019 18:55:35 +0800</lastBuildDate>
    
	<atom:link href="http://localhost:1313/tags/hive/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Excel数据导入Hive解决中文乱码</title>
      <link>http://localhost:1313/post/excel%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5hive%E8%A7%A3%E5%86%B3%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/</link>
      <pubDate>Fri, 19 Jul 2019 18:55:35 +0800</pubDate>
      
      <guid>http://localhost:1313/post/excel%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5hive%E8%A7%A3%E5%86%B3%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81/</guid>
      <description>Excel导hive： hive建表； 将Excel 导出另存为文本文档（制表符分隔）； 上传到Linux上：SecureCRT 可用rz命令； 更改编</description>
    </item>
    
    <item>
      <title>Hive数据导入Mysql</title>
      <link>http://localhost:1313/post/hive%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5mysql/</link>
      <pubDate>Fri, 19 Jul 2019 18:51:31 +0800</pubDate>
      
      <guid>http://localhost:1313/post/hive%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5mysql/</guid>
      <description>方法一（适用于批处理，调度）： 利用sqoop工具： 1 2 3 4 5 6 sqoop export --connectjdbc:mysql://192.168.56.104:3306/test?useSSL=false \ --username root \ --password123456 \ --table t2 \ --export-dir /user/hive/warehouse/test.db/mysql_t1 \ 更多详情可参见sqoop官网： http://sqoop.apache.org/docs/1.99.7/user.html 方法二（适用于临</description>
    </item>
    
    <item>
      <title>将Hive查询数据导出到本地</title>
      <link>http://localhost:1313/post/%E5%B0%86hive%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA%E5%88%B0%E6%9C%AC%E5%9C%B0/</link>
      <pubDate>Fri, 19 Jul 2019 18:38:39 +0800</pubDate>
      
      <guid>http://localhost:1313/post/%E5%B0%86hive%E6%9F%A5%E8%AF%A2%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%87%BA%E5%88%B0%E6%9C%AC%E5%9C%B0/</guid>
      <description>1.在Hive上执行： 1 2 3 4 set mapred.reduce.tasks =50; insert overwrite directory &amp;#39;/user/hdfspath/part_0000&amp;#39; ROW FORMAT DELIMITED FIELDS TERMINATED BY &amp;#39;,&amp;#39; select ... 2.将hdfs上的文件拷贝到Linux本地： 1 sudo -u user hadoop fs -get /user/hdfspath/part_0000 /tmp/a.txt 3.将Linux</description>
    </item>
    
    <item>
      <title>将HDFS文件加载到hive分区表</title>
      <link>http://localhost:1313/post/%E5%B0%86hdfs%E6%96%87%E4%BB%B6%E5%8A%A0%E8%BD%BD%E5%88%B0hive%E5%88%86%E5%8C%BA%E8%A1%A8/</link>
      <pubDate>Fri, 19 Jul 2019 18:27:36 +0800</pubDate>
      
      <guid>http://localhost:1313/post/%E5%B0%86hdfs%E6%96%87%E4%BB%B6%E5%8A%A0%E8%BD%BD%E5%88%B0hive%E5%88%86%E5%8C%BA%E8%A1%A8/</guid>
      <description>shell脚本如下： 如果目标表有分区，先清除分区 alter table table_name drop partition (dt &amp;gt;= &amp;lsquo;20181211&amp;rsquo;); 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/bash cu_date=`date +%Y%m%d` begin_date=&amp;#34;20181211&amp;#34; while [ &amp;#34;$begin_date&amp;#34; -le &amp;#34;$cu_date&amp;#34; ]; do echo &amp;#34;${begin_date}&amp;#34; hive -e &amp;#34;alter table table_name add partition (dt=&amp;#39;${begin_date}&amp;#39;) locatition &amp;#39;hdfs://Cluster/user/hive/warehouse/.../dt=${begin_date}&amp;#39;; &amp;#34;</description>
    </item>
    
  </channel>
</rss>